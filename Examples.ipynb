{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba7ed4b4",
   "metadata": {},
   "source": [
    "## Files used in bertFuncs.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48e420e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertFuncs import analyzeWord, getBert\n",
    "from createDims import createPolarDimension\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import string\n",
    "import ast\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8a9ed2",
   "metadata": {},
   "source": [
    "## Creating the required lookup files\n",
    "\n",
    "The lookup files are needed to set up the POLAR dimensions and to match these dimensions to word sense definitions and example sentences, when analyzing the result.\n",
    "\n",
    "The function ``create_lookup_files`` takes a list of lists as input. Each inner list contains a polar sense pair, where each word sense must be in WordNet readable format e.g. ``cold.a.01``), in orderder to automatically retrieve definitions and example sentences. All lookup files will be stored in the folder ``lookup_path``. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01d8e183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "def get_name(antonym):\n",
    "    return wn.synset(antonym).lemma_names()[0]\n",
    "\n",
    "def get_examples(antonym):\n",
    "    examples = wn.synset(antonym).examples()\n",
    "    # replace punctuation symbols with spaces\n",
    "    examples = [sent.translate(str.maketrans({k: \" \" for k in string.punctuation})) for sent in examples]\n",
    "    # add a space after each sentence\n",
    "    return ['{} '.format(sent) for sent in examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "958ab34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lookup_files(antonyms, lookup_path):\n",
    "    if len(np.unique(antonyms, axis=0)) != len(antonyms):\n",
    "        print(\"Your antonym list contains duplicates. Please try again!\")\n",
    "        return\n",
    "    \n",
    "    # get all word sense definitions\n",
    "    synset_defs = [[wn.synset(anto).definition() for anto in pair] for pair in antonyms]\n",
    "    # get example sentences from wordnet\n",
    "    examples_readable = {str(pair):{get_name(anto): get_examples(anto) for anto in pair} for pair in antonyms}\n",
    "    examples_lookup = [[[get_name(anto), get_examples(anto)] for anto in pair] for pair in antonyms]\n",
    "    \n",
    "    # save \n",
    "    with open(out_path + 'lookup_synset_dict.txt', 'w') as t:\n",
    "        t.write(json.dumps(antonyms, indent=4))\n",
    "    with open(out_path + 'lookup_synset_dict.pkl', 'wb') as p:\n",
    "        pickle.dump(antonyms, p)\n",
    "    with open(lookup_path + 'lookup_synset_definition.txt', 'w') as t:\n",
    "        t.write(json.dumps(synset_defs, indent=4))  \n",
    "    with open(lookup_path + 'lookup_synset_definition.pkl', 'wb') as p:\n",
    "        pickle.dump(synset_defs, p)        \n",
    "    with open(lookup_path + 'antonym_wordnet_example_sentences_readable_extended.txt', 'w') as t:\n",
    "        t.write(json.dumps(examples_readable, indent=4))  \n",
    "    with open(lookup_path + 'lookup_anto_example_dict.txt', 'w') as t:\n",
    "        t.write(json.dumps(examples_lookup, indent=4))      \n",
    "    with open(lookup_path + 'lookup_anto_example_dict.pkl', 'wb') as p:\n",
    "        pickle.dump(examples_lookup, p)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd2f37b",
   "metadata": {},
   "source": [
    "## Example usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e77ad0",
   "metadata": {},
   "source": [
    "Polar dimensions should be __given__ as nested list of antonym pairs in wordnet representation (sense-annotated).\n",
    "\n",
    "_Example:_   \n",
    "`\n",
    "[\n",
    "    ['a_posteriori.a.01', 'a_priori.a.01'],\n",
    "    ['abaxial.a.01', 'adaxial.a.01'],\n",
    "    ['abridge.v.01', 'elaborate.v.01'],\n",
    "    ...\n",
    "]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59f200e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder in which all lookup files will be stored\n",
    "out_path = 'antonyms/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89588ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define 3 exemplary POLAR dimensions\n",
    "dims = [['cold.a.01', 'hot.a.01'], ['bad.a.01', 'good.a.01'], ['intelligent.a.01', 'unintelligent.a.01'], ['capable.a.01', 'incapable.a.01']]\n",
    "    \n",
    "# create all lookup files\n",
    "create_lookup_files(dims, out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f240e5be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# get the embedding model \n",
    "tokenizer, model = getBert()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25e2f2c",
   "metadata": {},
   "source": [
    "Create the POLAR matrix (for base change or projection) from a given set of antonyms. The antonyms and their example sentences are forwarded to an embedding model (here: BERT) from which the required embeddings and difference vectors are created. ``antonym_path`` specifies where the readable example sentence lookup file is currently stored. ``out_path`` specifies where the POLAR matrix should be stored.\n",
    "\n",
    "The corresponding function can be found in ``createDims.py`` which is not part of the official SensePOLAR repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24a2a4ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start forwarding the Polar opposites ...\n"
     ]
    }
   ],
   "source": [
    "# create the base change matrix (this might take some time)\n",
    "createPolarDimension(model, tokenizer, out_path=out_path, antonym_path=out_path + \"antonym_wordnet_example_sentences_readable_extended.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8b9e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base change does not work well with only few dimensions -> compare with projection\n",
    "antonym_path = out_path + \"polar_dimensions.pkl\"\n",
    "word = \"school\"\n",
    "context = \"school teaches you a lot of smart things\"\n",
    "analyzeWord(word, context, model=model,tokenizer=tokenizer, antonym_path=antonym_path, lookup_path=out_path, numberPolar=4) #method=\"projection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d81d9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing the word:  earth\n",
      "In the context of:  the earth is shaking\n",
      "Top:  1\n",
      "Dimension:  intelligent<------>unintelligent\n",
      "Definitions:  having the capacity for thought and reason especially to a high degree<------>lacking intelligence\n",
      "Value: -1.5982703\n",
      "\n",
      "\n",
      "Top:  2\n",
      "Dimension:  bad<------>good\n",
      "Definitions:  having undesirable or negative qualities<------>having desirable or positive qualities especially those suitable for a thing specified\n",
      "Value: -1.1490841\n",
      "\n",
      "\n",
      "Top:  3\n",
      "Dimension:  cold<------>hot\n",
      "Definitions:  having a low or inadequate temperature or feeling a sensation of coldness or having been made cold by e.g. ice or refrigeration<------>used of physical heat; having a high or higher than desirable temperature or giving off heat or feeling or causing a sensation of heat or burning\n",
      "Value: -0.3484925\n",
      "\n",
      "\n",
      "Top:  4\n",
      "Dimension:  capable<------>incapable\n",
      "Definitions:  (usually followed by `of') having capacity or ability<------>(followed by `of') lacking capacity or ability\n",
      "Value:                      0.22581914\n",
      "\n",
      "\n",
      "[('intelligent', 'unintelligent'), ('bad', 'good'), ('cold', 'hot'), ('capable', 'incapable')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([('intelligent', 'unintelligent'),\n",
       "  ('bad', 'good'),\n",
       "  ('cold', 'hot'),\n",
       "  ('capable', 'incapable')],\n",
       " array([-0.3484925 , -1.1490841 , -1.5982703 ,  0.22581914], dtype=float32))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "antonym_path = out_path + \"polar_dimensions.pkl\"\n",
    "word = \"earth\"\n",
    "context = \"the earth is shaking\"\n",
    "\n",
    "analyzeWord(word, context, model=model, tokenizer=tokenizer, antonym_path=antonym_path, lookup_path=out_path, numberPolar=4, method=\"projection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "666aafba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synonyms for \"happy\": {'happy', 'well-chosen', 'felicitous', 'glad'}\n",
      "Antonyms for \"happy\": {'unhappy'}\n",
      "Examples for \"happy\": ['a happy smile', 'spent many happy days on the beach', 'a happy marriage', 'a felicitous life', 'a happy outcome', 'glad to help', 'a happy turn of phrase', 'a few well-chosen words']\n",
      "Definitions for \"happy\": {'eagerly disposed to act or to be of service', 'well expressed and to the point', 'enjoying or showing or marked by joy or pleasure', 'marked by good fortune'}\n"
     ]
    }
   ],
   "source": [
    "from dictionaryapi import Dictionary\n",
    "\n",
    "wordnet = Dictionary('wordnet')\n",
    "\n",
    "# Get synonyms for \"happy\"\n",
    "synonyms = wordnet.get_synonyms('happy')\n",
    "print(f'Synonyms for \"happy\": {synonyms}')\n",
    "\n",
    "# Get antonyms for \"happy\"\n",
    "antonyms = wordnet.get_antonyms('happy')\n",
    "print(f'Antonyms for \"happy\": {antonyms}')\n",
    "\n",
    "# Get examples for \"happy\" (always empty for WordNet)\n",
    "examples = wordnet.get_examples('happy')\n",
    "print(f'Examples for \"happy\": {examples}')\n",
    "\n",
    "# Get definitions for \"happy\"\n",
    "definitions = wordnet.get_definition('happy')\n",
    "print(f'Definitions for \"happy\": {definitions}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec191eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synonyms for \"happy\": None\n",
      "Antonyms for \"happy\": None\n",
      "Examples for \"happy\": None\n",
      "Definitions for \"happy\": None\n"
     ]
    }
   ],
   "source": [
    "dictionaryapi = Dictionary('dictionaryapi', api_key='b4b51989-1b9d-4690-8975-4a83df13efc4')\n",
    "\n",
    "# Get synonyms for \"happy\"\n",
    "synonyms = dictionaryapi.get_synonyms('happy')\n",
    "print(f'Synonyms for \"happy\": {synonyms}')\n",
    "\n",
    "# Get antonyms for \"happy\"\n",
    "antonyms = dictionaryapi.get_antonyms('happy')\n",
    "print(f'Antonyms for \"happy\": {antonyms}')\n",
    "\n",
    "# Get examples for \"happy\"\n",
    "examples = dictionaryapi.get_examples('happy')\n",
    "print(f'Examples for \"happy\": {examples}')\n",
    "\n",
    "# Get definitions for \"happy\"\n",
    "definitions = dictionaryapi.get_definition('happy')\n",
    "print(f'Definitions for \"happy\": {definitions}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
