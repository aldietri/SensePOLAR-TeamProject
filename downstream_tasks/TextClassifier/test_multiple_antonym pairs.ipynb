{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bbca911",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_functions_v2 import analyzeWord, getBert\n",
    "from createDims import createPolarDimension\n",
    "from get_data_from_file import create_lookup_from_data_file,create_lookupFiles_out_of_adjectives_list_using_file\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import string\n",
    "import ast\n",
    "import torch\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe62f7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "def get_name(antonym):\n",
    "    return wn.synset(antonym).lemma_names()[0]\n",
    "\n",
    "#def get_examples(antonym):\n",
    "#    examples = wn.synset(antonym).examples()\n",
    "    # replace punctuation symbols with spaces\n",
    "#    examples = [sent.translate(str.maketrans({k: \" \" for k in string.punctuation})) for sent in examples]\n",
    "    # add a space after each sentence\n",
    "#    return ['{} '.format(sent) for sent in examples]\n",
    "\n",
    "\n",
    "def get_examples(antonym):\n",
    "    antonym= wn.synset(antonym)\n",
    "    examples=antonym.examples()\n",
    "    #save only examples that containt the required word\n",
    "    correct_examples=[]\n",
    "    for example in examples:\n",
    "        if re.search(r'\\b'+str(antonym.lemma_names()[0])+'\\\\b', example, re.I) is not None:\n",
    "            correct_examples.append(example)\n",
    "    \n",
    "\n",
    "    \n",
    "    examples = [sent.translate(str.maketrans({k: \" \" for k in string.punctuation})) for sent in correct_examples]\n",
    "    # add a space after each sentence\n",
    "    return ['{} '.format(sent) for sent in examples]\n",
    "\n",
    "def create_lookup_files(antonyms, lookup_path):\n",
    "    if len(np.unique(antonyms, axis=0)) != len(antonyms):\n",
    "        print(\"Your antonym list contains duplicates. Please try again!\")\n",
    "        return\n",
    "    \n",
    "    # get all word sense definitions\n",
    "    synset_defs = [[wn.synset(anto).definition() for anto in pair] for pair in antonyms]\n",
    "    # get example sentences from wordnet\n",
    "    examples_readable = {str(pair):{get_name(anto): get_examples(anto) for anto in pair} for pair in antonyms}\n",
    "    examples_lookup = [[[get_name(anto), get_examples(anto)] for anto in pair] for pair in antonyms]\n",
    "    \n",
    "    # save \n",
    "    with open(out_path + 'lookup_synset_dict.txt', 'w') as t:\n",
    "        t.write(json.dumps(antonyms, indent=4))\n",
    "    with open(out_path + 'lookup_synset_dict.pkl', 'wb') as p:\n",
    "        pickle.dump(antonyms, p)\n",
    "    with open(lookup_path + 'lookup_synset_definition.txt', 'w') as t:\n",
    "        t.write(json.dumps(synset_defs, indent=4))  \n",
    "    with open(lookup_path + 'lookup_synset_definition.pkl', 'wb') as p:\n",
    "        pickle.dump(synset_defs, p)        \n",
    "    with open(lookup_path + 'antonym_wordnet_example_sentences_readable_extended.txt', 'w') as t:\n",
    "        t.write(json.dumps(examples_readable, indent=4))  \n",
    "    with open(lookup_path + 'lookup_anto_example_dict.txt', 'w') as t:\n",
    "        t.write(json.dumps(examples_lookup, indent=4))      \n",
    "    with open(lookup_path + 'lookup_anto_example_dict.pkl', 'wb') as p:\n",
    "        pickle.dump(examples_lookup, p)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a0b0490",
   "metadata": {},
   "outputs": [],
   "source": [
    "dims=[['nasty.a.01', 'nice.a.01'],\n",
    " ['better.a.01', 'worse.a.01'],\n",
    " ['better.a.02', 'worse.a.02'],\n",
    " ['bad.a.01', 'good.a.01'],\n",
    " ['ill.a.01', 'well.a.01'],\n",
    " ['ill.r.01', 'well.r.01'],\n",
    " ['ill.r.02', 'well.r.06'],\n",
    " ['badly.r.07', 'well.r.10'],\n",
    " ['badly.r.08', 'well.r.11'],\n",
    " ['badly.r.10', 'well.r.13'],\n",
    " ['bloodless.a.02', 'bloody.a.01'],\n",
    " ['brave.n.02', 'timid.n.01'],\n",
    " ['brave.a.01', 'cowardly.a.01'],\n",
    " ['busy.a.01', 'idle.a.01'],\n",
    " ['cautious.a.01', 'incautious.a.01'],\n",
    " ['clean.a.01', 'dirty.a.01'],\n",
    " ['clean.a.07', 'dirty.a.04'],\n",
    " ['clean.a.08', 'dirty.a.02'],\n",
    " ['fairly.r.03', 'unfairly.r.01'],\n",
    " ['clear.a.11', 'cloudy.a.02'],\n",
    " ['crowded.a.01', 'uncrowded.a.01'],\n",
    " ['dead.n.01', 'living.n.02'],\n",
    " ['alive.a.01', 'dead.a.01'],\n",
    " ['dead.a.02', 'live.a.02'],\n",
    " ['different.a.01', 'same.a.02'],\n",
    " ['like.a.01', 'unlike.a.01'],\n",
    " ['distinct.a.02', 'indistinct.a.01'],\n",
    " ['eager.a.01', 'uneager.a.01'],\n",
    " ['discouraging.a.01', 'encouraging.a.01'],\n",
    " ['cheap.a.01', 'expensive.a.01'],\n",
    " ['faithful.a.01', 'unfaithful.a.01'],\n",
    " ['faithful.a.03', 'unfaithful.a.02'],\n",
    " ['coarse.a.01', 'fine.a.05'],\n",
    " ['frail.a.01', 'robust.a.01'],\n",
    " ['glorious.a.01', 'inglorious.a.02'],\n",
    " ['healthy.a.01', 'unhealthy.a.01'],\n",
    " ['reasonably.r.01', 'unreasonably.r.02'],\n",
    " ['lucky.a.02', 'unlucky.a.01'],\n",
    " ['imperfect.a.01', 'perfect.a.01'],\n",
    " ['real.a.01', 'unreal.a.01'],\n",
    " ['real.a.02', 'unreal.a.02'],\n",
    " ['nominal.a.04', 'real.a.06'],\n",
    " ['insubstantial.a.01', 'substantial.a.03'],\n",
    " ['poor.a.02', 'rich.a.01'],\n",
    " ['poor.a.04', 'rich.a.02'],\n",
    " ['lean.a.02', 'rich.a.07'],\n",
    " ['poor.a.03', 'rich.a.08'],\n",
    " ['glazed.a.03', 'unglazed.a.02'],\n",
    " ['intelligent.a.01', 'unintelligent.a.01'],\n",
    " ['tender.a.01', 'tough.a.01'],\n",
    " ['tender.a.06', 'tough.a.03'],\n",
    " ['grateful.a.01', 'ungrateful.a.01'],\n",
    " ['rested.a.01', 'tired.a.01'],\n",
    " ['beautiful.a.01', 'ugly.a.01'],\n",
    " ['right.n.07', 'wrong.n.01'],\n",
    " ['correct.a.01', 'incorrect.a.01'],\n",
    " ['right.a.04', 'wrong.a.02'],\n",
    " ['right.a.05', 'wrong.a.05'],\n",
    " ['correctly.r.01', 'incorrectly.r.02'],\n",
    " ['attractive.a.01', 'unattractive.a.01'],\n",
    " ['attractive.a.03', 'repulsive.a.02'],\n",
    " ['bad.n.01', 'good.n.03'],\n",
    " ['regretful.a.01', 'unregretful.a.01'],\n",
    " ['breakable.a.01', 'unbreakable.a.01'],\n",
    " ['calm.a.02', 'stormy.a.01'],\n",
    " ['clear.a.01', 'unclear.a.02'],\n",
    " ['clear.a.04', 'opaque.a.01'],\n",
    "\n",
    " ['comfortable.a.01', 'uncomfortable.a.02'],\n",
    " ['comfortable.a.02', 'uncomfortable.a.01'],\n",
    " ['dangerous.a.01', 'safe.a.01'],\n",
    " ['defeated.a.01', 'undefeated.a.01'],\n",
    " ['difficult.a.01', 'easy.a.01'],\n",
    "\n",
    " ['quickly.r.01', 'slowly.r.01'],\n",
    " ['energetic.a.01', 'lethargic.a.01'],\n",
    " ['evil.n.03', 'good.n.02'],\n",
    " ['foolish.a.01', 'wise.a.01'],\n",
    " ['hungry.a.01', 'thirsty.a.02'],\n",
    " ['important.a.01', 'unimportant.a.01'],\n",
    " ['guilty.a.01', 'innocent.a.01'],\n",
    " ['joyless.a.01', 'joyous.a.01'],\n",
    " ['heavy.a.01', 'light.a.01'],\n",
    " ['dark.a.02', 'light.a.02'],\n",
    " ['heavy.a.03', 'light.a.03'],\n",
    " ['heavy.a.02', 'light.a.04'],\n",
    " ['heavy.a.04', 'light.a.05'],\n",
    " ['dark.a.01', 'light.a.06'],\n",
    " ['heavy.a.08', 'light.a.13'],\n",
    " ['heavy.a.09', 'light.a.14'],\n",
    " ['long.a.01', 'short.a.01'],\n",
    " ['long.a.02', 'short.a.02'],\n",
    " ['long.a.05', 'short.a.06'],\n",
    "\n",
    " ['disobedient.a.01', 'obedient.a.01'],\n",
    " ['fancy.a.01', 'plain.a.02'],\n",
    " ['confident.a.01', 'diffident.a.02'],\n",
    " ['sparkling.a.02', 'still.a.05'],\n",
    " ['tame.a.02', 'wild.a.01'],\n",
    " ['tame.a.03', 'wild.a.02'],\n",
    " ['lax.a.03', 'tense.a.03'],\n",
    " ['thoughtful.a.02', 'thoughtless.a.01'],\n",
    " ['heedful.a.01', 'heedless.a.01'],\n",
    " ['unusual.a.01', 'usual.a.01'],\n",
    " ['familiar.a.02', 'strange.a.01'],\n",
    " ['black.a.01', 'white.a.01'],\n",
    " ['black.a.02', 'white.a.02'],\n",
    " ['bright.a.01', 'dull.a.02'],\n",
    " ['dimmed.a.01', 'undimmed.a.01'],\n",
    " ['careful.a.01', 'careless.a.01'],\n",
    " ['cheerful.a.01', 'depressing.a.01'],\n",
    " ['colorful.a.02', 'colorless.a.01'],\n",
    " ['colored.a.01', 'uncolored.a.01'],\n",
    " ['concerned.a.01', 'unconcerned.a.01'],\n",
    " ['cooperative.a.02', 'uncooperative.a.01'],\n",
    " ['curious.a.02', 'incurious.a.01'],\n",
    " ['compliant.a.01', 'defiant.a.01'],\n",
    " ['dull.a.01', 'lively.a.01'],\n",
    " ['dull.a.06', 'sharp.a.08'],\n",
    " ['dull.a.09', 'sharp.a.09'],\n",
    " ['dejected.a.01', 'elated.a.01'],\n",
    " ['enthusiastic.a.01', 'unenthusiastic.a.01'],\n",
    " ['fair.a.01', 'unfair.a.01'],\n",
    " ['friendly.a.01', 'unfriendly.a.02'],\n",
    " ['friendly.a.03', 'unfriendly.a.01'],\n",
    " ['friendly.a.04', 'hostile.a.02'],\n",
    " ['happy.a.01', 'unhappy.a.01'],\n",
    " ['impossible.a.01', 'possible.a.01'],\n",
    " ['kind.a.01', 'unkind.a.01'],\n",
    " ['open.a.01', 'shut.a.01'],\n",
    " ['closed.a.01', 'open.a.02'],\n",
    " ['closed.a.04', 'open.a.05'],\n",
    " ['covert.a.01', 'overt.a.01'],\n",
    " ['pleasant.a.01', 'unpleasant.a.01'],\n",
    " ['humble.a.02', 'proud.a.01'],\n",
    " ['foreign.a.02', 'native.a.01'],\n",
    " ['troubled.a.01', 'untroubled.a.01'],\n",
    " ['interested.a.01', 'uninterested.a.01']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f581f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path='all_antonyms/'\n",
    "create_lookup_files(dims, out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8b5330b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model imported\n"
     ]
    }
   ],
   "source": [
    "tokenizer, model = getBert()\n",
    "print(\"Model imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47f099c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start forwarding the Polar opposites ...\n"
     ]
    }
   ],
   "source": [
    "createPolarDimension(model, tokenizer, out_path=out_path, antonym_path=out_path + \"antonym_wordnet_example_sentences_readable_extended.txt\")\n",
    "print(\"dimensions created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ced7046",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
